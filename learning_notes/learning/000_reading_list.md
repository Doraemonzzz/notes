# Reading List

## 综述
1. Review of stability properties of neural plasticity rules for implementation on memristive neuromorphic hardware
   1. 好文
2. 

## Dataset
1. Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions
2. 

## Fast weight
1. Neural Differential Equations for Learning to Program Neural Nets Through Continuous Learning Rules
   1. 给出Hebb, Oja, Delta rule的连续形式以及linear attention。
2. Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions
   1. 用Fast weight算q, k, v, 递归concat。
3. A Modern Self-Referential Weight Matrix That Learns to Modify Itself
   1. 用Fast weight算q, k, v。
4. Images as Weight Matrices: Sequential Image Generation Through Synaptic Learning Rules
   1. Fast weight生成图像。
5. 

## Hebbian

1. [PyTorch-Hebbian: facilitating local learning in a deep learning framework](https://github.com/julestalloen/pytorch-hebbian/)
2. https://github.com/DimaKrotov/Biological_Learning
3. Torch support: https://github.com/heydarshahi/Biological_Learning
4. HebbNet: A Simplified Hebbian Learning Framework to do Biologically Plausible Learning
5. Is Bio-Inspired Learning Better than Backprop? Benchmarking Bio Learning vs. Backprop


