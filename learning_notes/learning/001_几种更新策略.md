# 几种学习策略

一些遗留的方法: Feedback Alignment，Direct Feedback Alignment, 见Is Bio-Inspired Learning Better than Backprop? Benchmarking Bio Learning vs. Backprop

综述见:
Review of stability properties of neural plasticity rules for implementation on memristive neuromorphic hardware

基本记号：
$$
y_k =  \mathbf w_k^\top \mathbf x, \mathbf x,  \mathbf w_k \in \mathbb R^{d},  y_k \in \mathbb R,\\

\mathbf y =  \mathbf W^\top \mathbf x, \mathbf W \in \mathbb R^{d\times e}, \mathbf x \in \mathbb R^d, \mathbf y \in \mathbb R^e,  \\

\mathbf y^\top = [y_1,\ldots , y_e], \mathbf W^\top  =[\mathbf w_1, \ldots, \mathbf w_e]
$$

## [Hebb Rule](https://en.wikipedia.org/wiki/Hebbian_theory)

$$
y_k = \mathbf w_k^\top \mathbf x,  
\Delta \mathbf w_k = \alpha y_k \mathbf x, 
\mathbf w_{t+1}=\mathbf w_t +  \alpha y_k \mathbf x. \\
\mathbf y =\mathbf W ^\top \mathbf x, \Delta\mathbf W= \alpha \mathbf x^\top \mathbf y, 
\mathbf W_{t+1}=\mathbf W_t +  \mathbf x^\top \mathbf y.
$$

考虑模长：
$$
\begin{aligned}
\frac{d \mathbf w_k}{d t}&= y_k \mathbf x \\
&=  \mathbf x \mathbf x^\top \mathbf w_k,  \\
\frac 1 2 \frac{d (\mathbf w_k^\top\mathbf  w_k)}{dt}
&= \mathbf  w_k^\top \frac{d \mathbf w_k}{dt} \\
&=\alpha \mathbf  w_k^\top  y_k \mathbf x  \\
&= \alpha \mathbf  w_k^\top (\mathbf x^\top  \mathbf  w_k) \mathbf x  \\
&=\alpha \mathbf  w_k^\top \mathbf x  (\mathbf x^\top  \mathbf  w_k) \\
& =\alpha (\mathbf w_k^\top \mathbf x)^2 \\
&\ge 0, \\
\frac{d \mathbf W}{d t}&= \mathbf y^\top \mathbf x,  \\
\frac 1 2 \frac{d \mathrm {Trace}(\mathbf W^\top \mathbf W)}{dt}
&= \frac 1 2 \frac{d \left(\sum_{k} \mathbf w_k^\top\mathbf  w_k  \right)}{dt} \\
&=\alpha \sum_k (\mathbf w_k^\top \mathbf x)^2 \\
&\ge 0   .
\end{aligned}
$$

## [Krotov](https://github.com/DimaKrotov/Biological_Learning)


$$
\begin{aligned}
\frac{d \mathbf w_k}{d t}
&= g(y_k)\left( \mathbf x - y_k \mathbf x \right), \\
\frac{d \mathbf W}{d t}
&= g(y)^\top  \mathbf x - \mathrm{diag}\{g(y)\} g(y)^\top \mathbf x,  \\
g(h)&= \begin{cases}0, & h<0 \\ -\Delta, & 0 \leq h<h_* \\ 1, & h_* \leq h\end{cases}.

\end{aligned}
$$
实际实现时，$g(h)$取top 2即可。



## [Extended Hebb’s Rule / Covariance Rule](https://scholar.harvard.edu/files/binxuw/files/section_10_hebbian_learning_rules.pdf)

$$
\Delta w=\alpha y{{x}-\bar x} 
$$


## HebbNet

$$
\Delta w_{i j}^t=\eta^t \begin{cases}x_i^t z_j^t & \text { if } t=1 \\ x_i^t z_j^t-\frac{1}{t-1} \sum_{k=1}^{t-1} x_i^k z_j^k & \text { otherwise }\end{cases}  \\

\Delta \mathbf{w}= \begin{cases}\Delta w_l \mid \Delta w_l \in \Delta \mathbf{w} & \text { if } \Delta w_l \geq m \\ 0 & \text { otherwise }\end{cases}
$$


## Hebb + pasive decay
$$
\frac{d \mathbf w_k}{d t}
=  y_k \mathbf x -  \alpha \mathbf w_k , \\

\frac{d \mathbf W}{d t}
= \mathbf x^\top  \mathbf  y -   \alpha \mathbf W  . \\
$$

## [Instar Rule](https://arxiv.org/abs/2212.04614)
$$
\frac{d \mathbf w_k}{d t}
=  y_k \mathbf x -  y_k \mathbf w_k , \\

\frac{d \mathbf W}{d t}
= \mathbf x^\top  \mathbf  y -   \mathbf W \mathrm{diag}\{ y \} . \\

$$



## Outstar Rule

$$

\frac{d \mathbf w_k}{d t}
=  y_k \mathbf x -  \mathrm{diag}\{\mathbf x\}\mathbf w_k , \\

\frac{d \mathbf W}{d t}
= \mathbf x^\top  \mathbf  y -   \mathrm{diag}\{\mathbf  x  \}\mathbf W  . \\

$$

## Dual OR Rule

$$
\frac{d \mathbf w_k}{d t}
=  y_k \mathbf x -  y_k \mathbf w_k-\mathrm{diag}\{\mathbf x\} \mathbf w_k , \\

\frac{d \mathbf W}{d t}
= \mathbf x^\top  \mathbf  y -   \mathbf W \mathrm{diag}\{ y \}-\mathrm{diag}\{ x \}\mathbf W  . \\

$$

## Dual And Rule
$$
\frac{d \mathbf w_k}{d t}
=  y_k \mathbf x -  y_k\mathrm{diag}\{\mathbf x\} \mathbf w_k, \\

\frac{d \mathbf W}{d t}
= \mathbf x^\top  \mathbf  y -  \mathrm{diag}\{ x \} \mathbf W \mathrm{diag}\{ y \}  . \\

$$

## Oja’s rule

$$
\frac{d \mathbf w_k}{d t}
=  y_k \mathbf x -  y_k^2 \mathbf w_k , \\

\frac{d \mathbf W}{d t}
= (\mathbf x   -   \mathbf W \mathbf y )^\top \mathbf y . \\
$$

length normalization

$$
w=(w+\alpha yx), w=w/\| w\|. \\
$$
泰勒展开后近似于:
$$
w(t+1) = w(t)+\alpha y(x - yw(t)).
$$

w收敛于$X^TX$的最大特征值对应的特征向量。

## Sanger rule
$$
\Delta w_i = \alpha y_i (x - \sum_{k\le i} y_k w_k).  \\
w\in \mathbb R^{d\times e}
$$

## Oja multi-unit rule
$$
\Delta w_i = \alpha y_i (x - \sum_{k} y_k w_k).
$$

## Generalized Hebbian algorithm Sanger's rule
$$
\Delta w_{i j}=\eta\left(y_i x_j-y_i \sum_{k=1}^i w_{k j} y_k\right)
$$

## Pseudoinverse learning rule
https://www.lri.fr/~marc/EEAAX/Neurones/tutorial/pseudoinv/html/index.html
https://www.westernsydney.edu.au/__data/assets/pdf_file/0003/783156/Tapson,_van_Schaik_-_2013_-_Learning_the_pseudoinverse_solution_to_network_weights.pdf


# 参考资料

## 课程
1. https://bernstein-network.de/en/teaching-and-research/study-and-training/online-learning/theoretical-neuroscience-i/
2. https://bernstein-network.de/en/teaching-and-research/study-and-training/online-learning/theoretical-neuroscience-ii/
3. https://www.cs.cmu.edu/afs/cs/academic/class/15883-f21
4. https://www.cs.cmu.edu/afs/cs/academic/class/15883-f13
5. https://www.cs.cmu.edu/afs/cs/academic/class/15883-f21/slides/matrix-memory.pdf

## 综述
1. https://www.mit.edu/~9.54/fall14/slides/Class11.pdf
2. Review of stability properties of neural plasticity rules for implementation on memristive neuromorphic hardware
   1. Review_of_stability_properties_of_neural.pdf
3. Is Bio-Inspired Learning Better than Backprop? Benchmarking Bio Learning vs. Backprop
4. 

## 代码
1. https://github.com/jsalbert/biotorch
2. https://github.com/julestalloen/pytorch-hebbian
3. https://github.com/raphaelholca/hebbianCNN
4. https://github.com/clps1291-bioplausnn/BioPlausibleLearning
5. 


## Sanger
1. https://en.wikipedia.org/wiki/Generalized_Hebbian_algorithm
2. https://courses.cs.washington.edu/courses/cse528/09sp/sanger_pca_nn.pdf


## Hebbian
1. https://scholar.harvard.edu/files/binxuw/files/section_10_hebbian_learning_rules.pdf
2. https://zhuanlan.zhihu.com/p/454272138
3. HebbNet: A Simplified Hebbian Learning Framework to do Biologically Plausible Learning
4. Hebbian learning with gradients: Hebbian convolutional neural networks with modern deep learning frameworks
   1. 利用local loss加求导解决。
5. 

## Krotov
1. https://github.com/DimaKrotov/Biological_Learning
2. https://github.com/heydarshahi/Biological_Learning
3. https://github.com/gatapia/unsupervised_bio_classifier

## Contrastive Hebbian learning
1. https://ics.uci.edu/~xhx/publications/chl_nc.pdf
2. https://inc.ucsd.edu/mplab/46/media/CHL90.pdf

## Oja
1. https://www.cnblogs.com/MTandHJ/p/10527998.html
2. 

## Instar


## Delta
1. Learning the pseudoinverse solution to network weights
   1. 求伪逆的online方法，和delta rule很像
2. 
